---
title: A generative model for vocabulary estimation
author: "Mike Frank"
date: "July 17, 2015"
output:
  html_document:
  highlight: tango
theme: spacelab
---

```{r setup, include=FALSE}
library(knitr)
opts_chunk$set(message=FALSE, warning=FALSE, cache=TRUE)
```

```{r, cache=FALSE, include=FALSE}
library(dplyr)
library(tidyr)
# library(purrr)
library(lazyeval)
library(broom)
library(readr)
library(ggplot2)
library(RCurl)
library(langcog)
library(wordbankr)
library(stringr)

theme_set(theme_bw(base_size = 14))
inv.logit <- boot::inv.logit
```

We're going to use Stan. Set some settings:

```{r}
library(rstan)
library(rstanmulticore)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
```

First, connect to the Wordbank database and pull out the English WS and WG data.

```{r}
wordbank <- connect_to_wordbank()

admins <- get_administration_data() %>%
  filter(language == "English")

items <- get_item_data() %>%
  filter(language == "English", type == "word") %>%
  select(item_id, form, category, lexical_category, item) %>%
  mutate(num_item_id = as.numeric(substr(item_id, 6, nchar(item_id))))

get_form_data <- function(input_language, input_form) {
  form_items <- filter(items, form == input_form)
  get_instrument_data(input_language, input_form, 
                      form_items$item_id, iteminfo=form_items) %>%
    mutate(produces = value == "produces",
           understands = value == "understands" | value == "produces") %>%
    select(-value) %>%
    gather(measure, value, produces, understands) %>%
    left_join(select(admins, data_id, age)) %>%
    filter(!is.na(age)) %>%
    mutate(form = input_form)
}

ws <- get_form_data("English", "WS") %>%
  filter(measure == "produces")
```

Now aggregate by item. 

```{r}
items <- ws %>%
  group_by(item, age) %>%
  summarise(produces = mean(value, na.rm=TRUE))
            
write_csv(items, "ws.csv")
```

Now arrange.

```{r}
ordered <- items %>%
  group_by() %>%
  arrange(age, desc(produces)) %>%
  group_by(age) %>%
  mutate(index = 1:n()) 
```

Plot this with a glm sinusoid like in the Mayor & Plunkett (2011) paper.

```{r}
qplot(index, produces, facets= ~age, 
      geom = "line",
      data = ordered) + 
  ylim(c(0,1)) + 
  geom_smooth(method = "glm", 
              family = "binomial",
              formula = y ~ x) 
```

Try a polynomial fit.

```{r}
qplot(index, produces, facets= ~age, 
      geom = "line",
      data = ordered) + 
  ylim(c(0,1)) + 
  geom_smooth(method = "lm", 
              family = "binomial",
              color = "red",
              formula = y ~ poly(x, 3)) 
```

Doesn't work that well for the younger ages, though looks fine later.

Note age interactions in the stats.

```{r}
mp.glm <- glm(produces ~ index * age, family = "binomial", data = ordered)
summary(mp.glm)
```

### Further thoughts on Mayor & Plunkett model

The Mayor & Plunkett (2011) model has two corrections. The first uses the parametric form of the logistic to fill in low-frequency words that are not on the CDI, the second uses the difference between diary study counts and CDI counts to find a multiplier for higher-frequency words that are omitted. 

I think there's a bit of a conceptual issue here, as these two corrections should essentially be the same thing - there are some words that are not on the CDI, and more of these are the low frequency/hard words. So really, it all is a correction for missing words. 

Also - the first correction, which depends on the parametric form of the logistic, is much much smaller than the second. Take a look at this. 

Equation 3:

$$
p(w_i) = 1 - \frac{1}{1 + e^{\frac{-(i-a)}{b}}}
$$

```{r}
a <- 600 # from figure 6c for a 30mo
b <- 180 # from figure 6c
ranks <- 0:3000
age <- 20
ys <- (1 - (1 / (1 + exp((-(ranks - a ))/b))))

qplot(ranks, ys, 
      geom = "line") + 
  geom_vline(xintercept = 680, lty = 2) +
  geom_polygon(aes(x = c(ranks[ranks > 680], 
                         rev(ranks[ranks > 680])),
                   y = c(ys[ranks > 680], 
                         rep(0, length(ranks[ranks > 680])))), 
               fill = "blue", alpha = .5) + 
  ylim(c(0,1))
```

So adding the gray area gives us

$$
V_{corr_1} = b \log (1 + e^(a / b))
$$

but then adding the second correction is just a multiplier on this:

$$
V_{corr_2} = \alpha * V_{corr_1}
$$

Note that (strikingly), M&P2011 never give their value of $\alpha$ in the text. I estimate it below so that I can make an estimate of what correction 2 actually looks like... 

```{r}
a <- 600 
b <- 180 
ranks <- 1:3000
age <- 20
ys <- (1 - (1 / (1 + exp((-(ranks - a ))/b))))

area.uncorrected <- sum(ys[1:680])
area.c1 <- b * log(1 + exp(a / b))
area.c2 <- 1142 # from lookup table A1
alpha = area.c2 / area.c1

ys2 <- (1 - (1 / (1 + exp((-(ranks - (a*alpha) ))/(alpha * b)))))
area.corrected <- sum(ys2)

qplot(ranks, ys, 
      geom = "line") + 
  geom_vline(xintercept = 680, lty = 2) +
  geom_polygon(aes(x = c(ranks[ranks > 680], 
                         rev(ranks[ranks > 680])),
                   y = c(ys[ranks > 680], 
                         rep(0, length(ranks[ranks > 680])))), 
               fill = "blue", alpha = .5) + 
  geom_line(aes(y = ys2), lty = 3) + 
  geom_polygon(aes(x = c(ranks, rev(ranks)), 
                y = c(ys2, rev(ys))), 
            fill = "red", 
            alpha = .5) + 
  ylim(c(0,1)) 
```

So you can see that the second correction dwarfs the first correction in size, and is really based on a few small diary studies. 

In sum, I'm worried about this model for a few reasons:

* The distributional form (logit) is clearly not correct, so using this distributional form for extrapolation may have bad consequences.
* The first and second corrections aren't conceptually distinct: they both concern missing words. Both have to do with the sampling of words on the CDI from the broader vocabulary. 
* The second correction, which does most of the work, is - for reasonable reasons of data etc.- assumed to be a strict multiplier, which makes it do a ton of work at the higher end of vocabulary.

McMurray Model
--------------

This is the normal distribution model from the McMurray (2007) "Defusing the Vocabulary Explosion" paper. Assume kids are $k$ and words $w$. Kids' abilities are a function of age ($a$). We model production probability $p$. 

$$
k_i \sim  \mathcal{N}(\mu_1, \sigma_1) \\
w_j \sim \mathcal{N}(\mu_2, \sigma_2) \\
k_i(a) = k_i * a \\
p(k_i, w_j, a) = k_i(a) > w_j \\
$$

Try this with arbitrary parameters and make the same plot as above. 

```{r}
n.words <- 500
n.kids <- 1000

difficulty <- rnorm(n = n.words, m = 100, sd = 20)

ability <- rnorm(n = n.kids, m = 4, sd = 1) 
age <- 16:30

sims <- expand.grid(difficulty = difficulty, ability = ability, age = age) %>%
  group_by(ability, age) %>%
  mutate(item = 1:n()) %>%
  group_by() %>%
  mutate(produces = difficulty < ability * age) %>%
  group_by(item, age) %>%
  summarise(produces = mean(produces)) %>%
  group_by() %>%
  arrange(age, desc(produces)) %>%
  group_by(age) %>%
  mutate(index = 1:n())
              
qplot(index, produces, facets = ~age, 
      geom = "line", 
      data = sims)
```

Cool - so this looks almost identical in distributional form! Just for kicks, let's plot these on top of one another. For now I'm just tweaking parameters, though all four ($\mu_1$, $\mu_2$, $\sigma_1$, and $\sigma_2$) matter to the fit. 
```{r}
n.words <- 680 # same as on the CDI
n.kids <- 500 # reasonable number, so the curves are smoothish

difficulty <- rnorm(n = n.words, m = 100, sd = 20)

ability <- rnorm(n = n.kids, m = 4, sd = 1) 
age <- 16:30

sims <- expand.grid(difficulty = difficulty, ability = ability, age = age) %>%
  group_by(ability, age) %>%
  mutate(item = 1:n()) %>%
  group_by() %>%
  mutate(produces = difficulty < ability * age) %>%
  group_by(item, age) %>%
  summarise(produces = mean(produces)) %>%
  group_by() %>%
  arrange(age, desc(produces)) %>%
  group_by(age) %>%
  mutate(index = 1:n())

sims$dataset <- "simulations"
ordered$dataset <- "empirical WS"
d <- bind_rows(select(sims, index, produces, age, dataset), 
               select(ordered, index, produces, age, dataset))

qplot(index, produces, facets = ~age, 
      geom = "line", 
      col = dataset,
      data = d)
```

Extending the McMurray Model
----------------------------

So if the McMurray model is more or less the right model of vocabulary growth, how do we use knowledge about its parameters to correct CDI data? Two points here. 

First, we have to define a process by which words are excluded and then estimate the parameters of the full model based on the CDI model. 

Second, to do this we're still going to need some facts about total vocabulary to compare to the CDI vocabulary estimates, so we'll still need a study like Robinson & Mervis (1999). 

So let's consider the McMurray model as the base of our generative model, having the steps:

1. Generate a vocabulary with normally distributed difficulties, 
2. Generate a set of kids with normally distributed learning rates, and 
3. Generate a set of words to be on an instrument. 

Let's consider a  model of selecting words for an instrument where the probability of selecting a word gradually decreases with its difficulty rank. The easiest words (e.g. momma, no, ball) will all be on the list, but only a smaller number of hard words will make it. 

$$
p_{inclusion}(i) \sim \operatorname{Bern} (\frac{1}{i^{\alpha}})
$$

So each word is included as a function of a Bernoulli draw (coin flip) with probability proportional to its rank.

```{r}
xs <- 1:100
ys <- 1 / xs^(1/4)
qplot(xs, ys)
```

So let's see what this looks like. We'll do the same generative process as before but this time with many more words, which we'll exclude at random. 

```{r}
n.words <- 4000
n.kids <- 100
alpha = .25

difficulty <- rnorm(n = n.words, m = 100, sd = 20)
ability <- rnorm(n = n.kids, m = 4, sd = 1) 
age <- 16:30

sims <- expand.grid(difficulty = difficulty, ability = ability, age = age) %>%
  group_by(ability, age) %>%
  mutate(item = 1:n()) %>%
  group_by() %>%
  mutate(produces = difficulty < ability * age) %>%
  group_by(item, age) %>%
  summarise(produces = mean(produces)) %>%
  group_by() %>%
  arrange(age, desc(produces)) %>%
  group_by(age) %>%
  mutate(index = 1:n())
```

Now choose the words to exclude, based on actual ground-truth difficulty.

```{r}
words.to.include <- expand.grid(difficulty = difficulty) %>%
  mutate(item = 1:n()) %>%
  arrange(difficulty) %>%
  mutate(diff.idx = 1:n()) %>%
  rowwise %>%
  mutate(inc = rbinom(1, size = 1, prob = 1/diff.idx^alpha) == 1) %>%
  filter(inc) %>%
  rowwise %>%
  mutate(sim.idx = sims$index[sims$item == item][1])
```

First visualize this with the words, with red being more included words. 

```{r}
qplot(index, produces, facets = ~age, 
      geom = "line", 
      data = sims) + 
  geom_vline(data = words.to.include, 
             aes(xintercept = sim.idx), 
             col = "red", alpha = .03)
```

Next show the curves with only those words included. 

```{r}
sims.subset <- sims %>%
  filter(item %in% words.to.include$item) %>%
  mutate(index = 1:n())

sims$dataset <- "full simulation"
sims.subset$dataset <- "item subset"

d <- bind_rows(sims, sims.subset)
  
qplot(index, produces, facets = ~age, 
      geom = "line", col = dataset,
      data = d) 
```

Now plot the multiplier in this simulation, which we can get based on the area under the curve. (As M&P2011 note, the area under the curve is the expected vocabulary estimate for that measure). 

```{r}
ms <- d %>%
  group_by(age, dataset) %>%
  summarise(produces = sum(produces))

qplot(age, produces, col = dataset, 
      geom = "line", data = ms) + 
  geom_hline(yintercept = 680, lty = 2) + 
  geom_hline(yintercept = 3000, lty = 2)
```

It's pretty clear that we're actually getting something similar to Bernard & Mervis (1999) here, and our correction isn't looking that dissimilar to M&P2011. Of course, this is all with six parameters I made up: $\mu_1$, $\mu_2$, $\sigma_1$, $\sigma_2$, $\alpha$ (the parameter on the words that got dropped), and $N_{total}$ (the total number of words we considered). But we (sort of) fit the first four to the empriical data. And the combo of $\alpha$ and $N_{total}$ wasn't totally invented either. 

But really the next step is to use the empirical data and B&M1999 (plus other data) to fit this model, since with the addition of some hyper-priors it's a well-defined generative model. 


Inferring Model Parameters
--------------------------

Let's start slow, first defining the McMurray model. 

```{r}
mcmurray <-'
data {                            
  int<lower=1> W; // number of words
  int<lower=1> S; // number of subjects
  int<lower=1> N; // number of datapoints (W*S)
  vector[S] age; // subject ages
  int<lower=0,upper=1> produces[N]; // actual data
}

parameters {
  real<lower=0> mu_w; // word mean
  real<lower=0> mu_s; // subject mean
  real<lower=0> sigma_w; // word SD
  real<lower=0> sigma_s; // subject SD
  real<lower=0> difficulty[W]; // word mean
  real<lower=0> ability[S]; // subject mean
}

model {
  mu_w ~ normal(0, 10); 
  mu_s ~ normal(0, 10);
  sigma_w ~ normal(0, 5);
  sigma_s ~ normal(0, 5);
  difficulty ~ normal(mu_w, sigma_w);
  ability ~ normal(mu_s, sigma_s);

  for (s in 1:S)
    for (w in 1:W)
      produces[W*(s-1) + w] ~ bernoulli_logit((ability[s] * age[s]) - difficulty[w]);
}
'
```

### Fake data simulations. 

Let's start by doing simulations with fake data. These are six kids and five words, with the kids gradually increasing in age. Their abilities should be lower and lower, and words 1, 3, and 5 should be estimated as hard.  

```{r}
dat <- list(age = c(12, 14, 16, 18, 20, 22), 
             produces = rep(c(0, 1, 0, 1, 0), 6), 
             W = 5, 
             S = 6, 
             N = 30)
```

Run!

```{r}
samps <- pstan(model_code = mcmurray, model_name = "mcmurray_simple", 
             data = dat, iter = 2000, warmup=200, thin=20, chains = 4, 
             pars = c('mu_w','mu_s','sigma_w', 'sigma_s', 
                      'difficulty', 'ability'), 
             verbose = F, refresh=2000/4) 
```

Now examine output. 

```{r}
samps
```

Looks reasonable to me. 

### Simulations with a subset of the data. 

I started out these simulations looking at many words, and a small number of kids, and the results looked funky. Not sure if that's a convergence issue, or an issue with not having the appropriate diversity of kids. So let's try a smaller number of words.

```{r}
n.words <- 6
n.subs <- 500
  
model.data <- ws %>%
  filter(data_id <= n.subs, 
         item %in% c("mommy","dog","up","table","truck","alligator")) %>%
  select(data_id, num_item_id, item, value, age) %>%
  rename(produces = value) %>%
  arrange(data_id) # critical to get the words nested inside subjects

# for simulations, fill in false for missing data
model.data$produces[is.na(model.data$produces)] <- FALSE 

ages <- model.data %>% 
              group_by(data_id) %>% 
              summarise(age = mean(age)) %>% 
              select(age) 

dat <- list(age = ages$age, 
            produces = model.data$produces, 
            W = n.words, 
            S = n.subs, 
            N = n.words * n.subs)

model.data %>%
  group_by(num_item_id) %>%
  summarise(produces = mean(produces))
```

Run the model.

```{r}
samps <- pstan(model_code = mcmurray, model_name = "mcmurray_simple", 
             data = dat, iter = 2000, warmup=200, thin=20, chains = 4, 
             pars = c('mu_w','mu_s','sigma_w', 'sigma_s', 
                      'difficulty', 'ability'), 
             verbose = F, refresh=2000/4) 
```

Diagnostics. Order of words is "alligator", "dog", "truck", "table", "mommy", "up."

```{r}
traceplot(samps, pars = c("mu_w", "mu_s", "sigma_w", "sigma_s"))
samplerpar = get_sampler_params(samps)[[1]]
summary(samplerpar)
pairs(samps, pars=c('mu_w', 'mu_s'))
```

### Simulations with full dataset

We need to use only complete cases. But even here, these simulations take a while.

```{r}
n.words <- 680

complete <- ws %>%
  group_by(data_id) %>%
  summarize(complete = sum(!is.na(value)) == n.words) %>%
  filter(complete)

n.subs  <- 10

model.data <- ws %>%
  select(data_id, num_item_id, item, value, age) %>%
  filter(data_id %in% complete$data_id) %>%
  mutate(data_id = as.numeric(as.factor(data_id))) %>% # hack to renumber
  filter(data_id <= n.subs) %>%
  rename(produces = value) %>%
  arrange(data_id) # critical to get the words nested inside subjects

ages <- model.data %>% 
              group_by(data_id) %>% 
              summarise(age = mean(age)) %>% 
              select(age) 

dat <- list(age = ages$age, 
            produces = model.data$produces, 
            W = n.words, 
            S = n.subs, 
            N = n.words * n.subs)
```

Run the model. Unfortunately this takes around 45 minutes, even for 100 subjects. So do it for 10 (~90s). 

```{r}
samps <- pstan(model_code = mcmurray, model_name = "mcmurray_simple", 
            data = dat, iter = 2000, warmup=200, thin=10, chains = 4, 
             pars = c('mu_w','mu_s','sigma_w', 'sigma_s', 
                      'difficulty', 'ability'), 
             verbose = F, refresh=2000/4) 
```

Re-merge coefficients with word data.

```{r}
coefs <- data.frame(summary(samps)$summary)
coefs$name <- rownames(coefs)

word_ids <- ws %>% 
  group_by(num_item_id) %>% 
  summarise(item = unique(item))

words <- coefs %>% 
  filter(str_detect(name, "difficulty")) %>%
  separate(name, c("name", "num_item_id"), "\\[") %>%
  mutate(num_item_id = as.numeric(str_replace(num_item_id, "]", ""))) %>%
  left_join(word_ids) %>%
  select(mean, se_mean, sd, item) %>%
  arrange(mean) %>%
  mutate(item = factor(item, 
                        levels = item,
                        labels = item))
```

And plot the first 100 words or so... even within this, you can see that there is some good signal here and we are learning good stuff.  

```{r}
qplot(item, mean, data=words[1:100,]) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

### Attempt #2: no by-subject coefficients

The problem here is computational complexity. Let's rewrite the model so that we're not actually learning anything about each subject, just learning a mean and variance that gets multiplied by age. 

```{r}
mcmurray_nosubs <-'
data {                            
  int<lower=1> W; // number of words
  int<lower=1> S; // number of subjects
  int<lower=1> N; // number of datapoints (W*S)
  vector[S] age; // subject ages
  int<lower=0,upper=1> produces[N]; // actual data
}

parameters {
  real<lower=0> mu_w; // word mean
  real<lower=0> mu_s; // subject mean
  real<lower=0> sigma_w; // word SD
  real<lower=0> difficulty[W]; // word mean
}

model {
  mu_w ~ normal(0, 10); 
  mu_s ~ normal(0, 10);
  sigma_w ~ normal(0, 5);
  difficulty ~ normal(mu_w, sigma_w);

  for (s in 1:S)
    for (w in 1:W)
      produces[W*(s-1) + w] ~ bernoulli_logit((mu_s * age[s]) - difficulty[w]);
}
'
```

Again, simulations with fake data. to begin These are six kids and five words, with the kids gradually increasing in age. Their abilities should be lower and lower, and words 1, 3, and 5 should be estimated as hard.  

```{r}
dat <- list(age = c(12, 14, 16, 18, 20, 22), 
             produces = rep(c(0, 1, 0, 1, 0), 6), 
             W = 5, 
             S = 6, 
             N = 30)
```

Run!

```{r}
samps <- pstan(model_code = mcmurray_nosubs, model_name = "mcmurray_nosubs", 
             data = dat, iter = 2000, warmup=200, thin=20, chains = 4, 
             pars = c('mu_w','mu_s','sigma_w', 'difficulty'), 
             verbose = F, refresh=2000/4) 
```

Now examine output. 

```{r}
samps
```

Looks fine. On to a subset of the dataset.

```{r}
n.words <- 680

complete <- ws %>%
  group_by(data_id) %>%
  summarize(complete = sum(!is.na(value)) == n.words) %>%
  filter(complete)

n.total.subs <- length(unique(ws$data_id))
n.subs  <- 100

model.data <- ws %>%
  select(data_id, num_item_id, item, value, age) %>%
  filter(data_id %in% complete$data_id) %>%
  mutate(data_id = as.numeric(as.factor(data_id))) %>% # hack to renumber
  filter(data_id %in% sample(n.total.subs)[1:n.subs]) %>%
  rename(produces = value) %>%
  arrange(data_id) # critical to get the words nested inside subjects

ages <- model.data %>% 
              group_by(data_id) %>% 
              summarise(age = mean(age)) %>% 
              select(age) 

dat <- list(age = ages$age,
            produces = model.data$produces, 
            W = n.words, 
            S = n.subs, 
            N = n.words * n.subs)

samps <- pstan(model_code = mcmurray_nosubs, model_name = "mcmurray_nosubs", 
            data = dat, iter = 1000, warmup=100, thin=10, chains = 4, 
             pars = c('mu_w','mu_s','sigma_w','difficulty'), 
             verbose = F, refresh=2000/4) 

samps
```

Now re-merge.

```{r}
coefs <- data.frame(summary(samps)$summary)
coefs$name <- rownames(coefs)

word_ids <- ws %>% 
  group_by(num_item_id) %>% 
  summarise(item = unique(item))

words <- coefs %>% 
  filter(str_detect(name, "difficulty")) %>%
  separate(name, c("name", "num_item_id"), "\\[") %>%
  mutate(num_item_id = as.numeric(str_replace(num_item_id, "]", ""))) %>%
  left_join(word_ids) %>%
  select(mean, se_mean, sd, item) %>%
  arrange(mean) %>%
  mutate(item = factor(item, 
                        levels = item,
                        labels = item))
```

and plot again:

```{r}
qplot(item, mean, data=words[1:50,]) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

So clearly this model is getting us something. The only worry is that it's still so slow. 

Let's try rewriting it so it is more vectorized? Hard to do under current Stan infrastructure. 

### Attempt #3: Binomials

But we can do an age aggregation and use binomials. (Since we're not fitting subjects). 

```{r}
mcmurray_binom <-'
data {                            
  int<lower=1> W; // number of words
  int<lower=1> A; // number of ages
  int<lower=1> N; // number of datapoints (W*A)
  vector[A] age; // subject ages
  int produces[N]; // count data
  int attempts[N]; // count data
}

parameters {
  real<lower=0> mu_w; // word mean
  real<lower=0> mu_s; // subject mean
  real<lower=0> sigma_w; // word SD
  real<lower=0> difficulty[W]; // word mean
}

model {
  mu_w ~ normal(0, 10); 
  mu_s ~ normal(0, 10);
  sigma_w ~ normal(0, 5);
  difficulty ~ normal(mu_w, sigma_w);

  for (a in 1:A)
    for (w in 1:W) 
      produces[W*(a-1) + w] ~ binomial_logit(attempts[W*(a-1) + w], (mu_s * age[a]) - difficulty[w]);
}
'
```

Now run this with the full dataset. 
Interestingly, thinning is not recommended by many authors: see [http://doingbayesiandataanalysis.blogspot.ca/2011/11/thinning-to-reduce-autocorrelation.html]

```{r}
n.words <- 680

complete <- ws %>%
  group_by(data_id) %>%
  summarize(complete = sum(!is.na(value)) == n.words) %>%
  filter(complete)


model.data <- ws %>%
  select(data_id, num_item_id, item, value, age) %>%
  filter(data_id %in% complete$data_id) %>%
  group_by(age, num_item_id) %>%
  summarise(n = sum(value), 
            N = length(value))

ages <- unique(model.data$age)

dat <- list(age = ages,
            produces = model.data$n, 
            attempts = model.data$N,             
            W = n.words, 
            A = length(ages), 
            N = n.words * length(ages))

ptm <- proc.time()
samps <- stan(model_code = mcmurray_binom, 
              cores = 4, 
              data = dat, iter = 300, warmup=100, thin=10, chains = 24, 
              pars = c("mu_w", "mu_s", "sigma_w", "difficulty")) 
proc.time() - ptm

```

Diagnostics.

```{r}
traceplot(samps, pars = c('mu_w','mu_s','sigma_w'))

```

Success!

```{r}
coefs <- data.frame(summary(samps)$summary)
coefs$name <- rownames(coefs)

word_ids <- ws %>% 
  group_by(num_item_id) %>% 
  summarise(item = unique(item))

words <- coefs %>% 
  filter(str_detect(name, "difficulty")) %>%
  separate(name, c("name", "num_item_id"), "\\[") %>%
  mutate(num_item_id = as.numeric(str_replace(num_item_id, "]", ""))) %>%
  left_join(word_ids) %>%
  select(mean, se_mean, sd, item) %>%
  arrange(mean) %>%
  mutate(item = factor(item, 
                        levels = item,
                        labels = item))
```

and plot again:

```{r}
qplot(mean, as.numeric(item), 
      geom = "point", data=words) + 
  geom_text(aes(x = mean + .2, label = item), 
            data = words[seq(1,680, 10),], 
            size = 2)
```

Check the histogram:

```{r}
qplot(mean, geom = "blank", data = words) + 
  geom_histogram(aes(y = ..density..)) + 
  geom_density(col = "red")
```

What are the words with means below 1?

```{r}
filter(words, mean < 1)
```
